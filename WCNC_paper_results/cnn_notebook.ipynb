{"cells":[{"cell_type":"code","execution_count":64,"metadata":{"executionInfo":{"elapsed":518,"status":"ok","timestamp":1728052793951,"user":{"displayName":"john sengendo","userId":"07878845531873393274"},"user_tz":-120},"id":"dCTTphj9IPYp"},"outputs":[],"source":["# Helper libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","import sklearn as sk\n","import pandas as pd\n","from pandas import read_csv\n","from datetime import datetime\n","import math\n","import os\n","\n","# fixing random seed for reproducibility\n","seed = 2022\n","np.random.seed(seed)\n"]},{"cell_type":"markdown","metadata":{"id":"2j5JmPmw4SSr"},"source":["# New Section"]},{"cell_type":"code","execution_count":65,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1728052794956,"user":{"displayName":"john sengendo","userId":"07878845531873393274"},"user_tz":-120},"id":"YaPqwQ0h4pU4","outputId":"b6db654d-544a-466b-aca8-d07928fbe81d"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.17.0\n"]}],"source":["# TensorFlow and tf.keras\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","print(tf.__version__)\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D,Input\n","from tensorflow.keras.layers import LSTM, GRU\n","from tensorflow.keras.regularizers import L1L2\n","from tensorflow.keras.constraints import max_norm\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras import optimizers\n","from tensorflow.keras.losses import MAE\n"]},{"cell_type":"code","execution_count":66,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3247,"status":"ok","timestamp":1728052798200,"user":{"displayName":"john sengendo","userId":"07878845531873393274"},"user_tz":-120},"id":"wBZjBeJnIFJO","outputId":"62116aef-03a5-437b-f3d6-e5d73998c968"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["\n","from google.colab import drive\n","drive.mount('/content/drive')\n","#\n","\n","##!ls '/content/gdrive/My Drive/'"]},{"cell_type":"code","execution_count":67,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1728052798201,"user":{"displayName":"john sengendo","userId":"07878845531873393274"},"user_tz":-120},"id":"xZg6m-sZTdii","outputId":"d20d4ef8-7de7-4ca0-83c6-998234097e07"},"outputs":[{"name":"stdout","output_type":"stream","text":["(809,)\n","(494,)\n","(495,)\n"]}],"source":["data = pd.read_csv('/content/drive/My Drive/pcap/packets_per_sec_analysis.csv')\n","\n","# Extracting the column 'packets_per_sec'\n","data = data['packets_per_sec']\n","\n","# Calculating the indices at which to split the data\n","train_split_index = int(len(data) * 0.45)\n","val_split_index = int(len(data) * 0.725)\n","\n","# Splitting the data into training, validation, and test datasets\n","train_dataset = data[:train_split_index]\n","val_dataset = data[train_split_index:val_split_index]\n","test_dataset = data[val_split_index:]\n","\n","print(train_dataset.shape)\n","print(val_dataset.shape)\n","print(test_dataset.shape)\n"]},{"cell_type":"code","execution_count":68,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1728052798201,"user":{"displayName":"john sengendo","userId":"07878845531873393274"},"user_tz":-120},"id":"gigu8o0lYxnp"},"outputs":[],"source":["# Selecting the column 'packets_per_sec' as the feature for the model\n","features = ['packets_per_sec']\n","\n","# train and validate\n","train_values = np.asarray(train_dataset.values, dtype=np.float32).reshape(-1, 1)\n","train_labels = np.asarray(train_dataset.values, dtype=np.float32)\n","\n","val_values = np.asarray(val_dataset.values, dtype=np.float32).reshape(-1, 1)\n","val_labels = np.asarray(val_dataset.values, dtype=np.float32)\n","\n","test_values = np.asarray(test_dataset.values, dtype=np.float32).reshape(-1, 1)\n","test_labels = np.asarray(test_dataset.values, dtype=np.float32)"]},{"cell_type":"code","execution_count":69,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1728052798201,"user":{"displayName":"john sengendo","userId":"07878845531873393274"},"user_tz":-120},"id":"kJASfvPcTVzN"},"outputs":[],"source":["# imports to show that there are many different scalers\n","# especially with recurrent NNs, the choice of scaler can make a difference\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import MaxAbsScaler\n","from sklearn.preprocessing import RobustScaler\n","from sklearn.preprocessing import Normalizer\n","from sklearn.preprocessing import QuantileTransformer\n","from sklearn.preprocessing import PowerTransformer\n","\n","\n","SS1 = StandardScaler()\n","SS1.fit(train_values)\n","\n","train_scaled = SS1.transform(train_values)\n","val_scaled = SS1.transform(val_values)\n","test_scaled = SS1.transform(test_values)"]},{"cell_type":"code","execution_count":70,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1728052798201,"user":{"displayName":"john sengendo","userId":"07878845531873393274"},"user_tz":-120},"id":"1tIqYyPdEON4"},"outputs":[],"source":["# window-based and recurrent networks:\n","\n","\"\"\"\n","This function creates a dataset for time-series analysis, specifically for window-based.\n","\n","Parameters:\n","- features: A numpy array containing the input data for the model.\n","- labels: A numpy array containing the corresponding output data for the model.\n","- ahead: An integer that specifies how many steps ahead in the future the labels are. Default is 4.\n","- window_size: An integer that specifies the size of the sliding window that is used to create the input data.\n","- max_window_size: An integer that specifies the maximum size of the sliding window.\n","\n","Returns:\n","- dataX: A 3D numpy array of shape (num_samples, window_size, num_features) containing the input data samples.\n","- labels: A 1D numpy array of shape (num_samples,) containing the corresponding labels for the input data samples.\n","\n","The function calculates the number of samples that can be created based on the size of the features array, the 'ahead' parameter, and the 'max_window_size' parameter. \n","It then creates a list of input data samples by sliding a window of size 'window_size' over the features array. Each input data sample is a 2D numpy array of shape (window_size, num_features), \n","where 'num_features' is the number of features in the features array. The function returns a tuple containing two numpy arrays: the first is a 3D numpy array of shape (num_samples, window_size, \n","num_features) containing the input data samples, and the second is a 1D numpy array of shape (num_samples,) containing the corresponding labels for the input data samples. \n","The labels are shifted 'ahead' steps into the future, and only the labels that correspond to the input data samples are included in the output array.\n","\"\"\"\n","\n","def create_dataset_windowed(features, labels, ahead=4, window_size=1, max_window_size=360):\n","    samples = features.shape[0] - ahead - (max_window_size - 1)\n","    window_size = min(max(window_size, 1), max_window_size)\n","\n","    dataX = np.array([features[(i + max_window_size - window_size):(i + max_window_size), :] for i in range(samples)])\n","    dataY = labels[ahead + max_window_size - 1 : ahead + max_window_size - 1 + samples]\n","\n","    return dataX, dataY"]},{"cell_type":"code","execution_count":71,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1728052798201,"user":{"displayName":"john sengendo","userId":"07878845531873393274"},"user_tz":-120},"id":"fR45InWra02f"},"outputs":[],"source":["def PlotResults(labels, predictions, window_size, ahead, dataset_type=\"\"):\n","    fig = plt.figure(figsize=(6, 4), dpi=300)\n","    ax1 = fig.add_subplot(111)\n","\n","    # Plotting the actual vs. predicted values\n","    ax1.plot(labels, 'k-', label='Observed traffic', linewidth=1)\n","    ax1.plot(predictions, 'r-', label='Predicted traffic', linewidth=1)\n","\n","    # Labeling the axes\n","    ax1.set_ylabel('Traffic (Packets per second)', fontsize=12)\n","    ax1.set_xlabel('Time (Seconds)', fontsize=12)\n","    # Adding the legend for clarification\n","    ax1.legend(loc='upper right', fontsize=10)\n","\n","    # Adding a more descriptive title\n","    ax1.set_title(f\"Comparison of Actual vs Predicted traffic\\n\"\n","                  f\"Look-back period: {window_size} Seconds | Forecasting horizon: {ahead} Seconds\", fontsize=14, pad=15)\n","\n","    # Showing the plot\n","    plt.tight_layout()  # Ensuring everything fits without overlapping\n","    plt.show()"]},{"cell_type":"code","execution_count":72,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1728052798201,"user":{"displayName":"john sengendo","userId":"07878845531873393274"},"user_tz":-120},"id":"JQe_OoLEaZA-"},"outputs":[],"source":["# plotting the loss curves\n","def plot_history(history):\n","  plt.figure(figsize = (6,4))\n","\n","  plt.xlabel('Epoch')\n","  plt.ylabel('Mae')\n","  plt.plot(history.epoch, np.array(history.history['mae']),'g-',\n","           label='Train MAE')\n","  plt.plot(history.epoch, np.array(history.history['val_mae']),'r-',\n","           label = 'Validation MAE')\n","  plt.legend()\n","  plt.show()"]},{"cell_type":"code","execution_count":73,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1TF51zpAEeZg3FXWFPRN3DgckgFzZousW"},"executionInfo":{"elapsed":395298,"status":"ok","timestamp":1728053193491,"user":{"displayName":"john sengendo","userId":"07878845531873393274"},"user_tz":-120},"id":"Msgighu2t956","outputId":"029f3955-a8fd-49cc-d37b-92b9476bcd89"},"outputs":[{"data":{"text/plain":["Output hidden; open in https://colab.research.google.com to view."]},"metadata":{},"output_type":"display_data"}],"source":["from sklearn.metrics import mean_absolute_error, mean_squared_error\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Input, Conv1D, Flatten, Dense, Activation\n","from tabulate import tabulate\n","\n","\"\"\"\n","The script trains a CNN model with different window sizes for prediction.\n","\n","The script uses the `create_dataset_windowed` function to create input data samples by sliding a window of size `window_size` over the features array.\n","The script then trains a CNN model with the input data samples and corresponding labels. The model is compiled with the Adam optimizer, mean absolute error (MAE) loss function, and MAE and MSE metrics.\n","The model is trained for 50 epochs with a batch size of 32. The training and validation MAE are calculated and stored in a pandas DataFrame.\n","The results are displayed as a table using the `tabulate` library and saved to a CSV file.\n","\n","Parameters:\n","- window_sizes: A list of integers specifying the window sizes to use for training the model.\n","- train_scaled: A numpy array containing the scaled input data for training the model.\n","- train_labels: A numpy array containing the corresponding output data for training the model.\n","- val_scaled: A numpy array containing the scaled input data for validating the model.\n","- val_labels: A numpy array containing the corresponding output data for validating the model.\n","\n","Returns:\n","- None\n","The script prints the model summary, training and validation MAE for each window size, and a table summarizing the results. The results are also saved to a CSV file.\n","\"\"\"\n","\n","histories = {}\n","\n","folder_path = '/content/drive/My Drive/pcap/'\n","\n","# Defining window sizes and ahead values\n","window_sizes = [60, 120, 180, 300]\n","ahead_values = [30, 60, 300, 600]\n","mae_results = pd.DataFrame()\n","mse_results = pd.DataFrame()\n","rmse_results = pd.DataFrame()\n","\n","for WINDOW in window_sizes:\n","    for AHEAD in ahead_values:\n","\n","        print(f\"Training model with window size: {WINDOW}\")\n","\n","        X_train_w, r_train_w = create_dataset_windowed(train_scaled, train_labels, window_size=WINDOW)\n","        X_val_w, r_val_w = create_dataset_windowed(val_scaled, val_labels, window_size=WINDOW)\n","        X_test_w, r_test_w = create_dataset_windowed(test_scaled, test_labels, window_size=WINDOW)\n","\n","        # Rebuild the model for each window size\n","        CNNmodel = Sequential()\n","\n","        # Adding input layer explicitly with Input()\n","        CNNmodel.add(Input(shape=(WINDOW, X_train_w.shape[-1])))\n","\n","        CNNmodel.add(Conv1D(filters=128, kernel_size=4, padding='same', activation='relu'))\n","        CNNmodel.add(Conv1D(filters=128, kernel_size=4, padding='same', activation='relu'))\n","        CNNmodel.add(Conv1D(filters=64, kernel_size=4, padding='same', activation='relu'))\n","        CNNmodel.add(Conv1D(filters=32, kernel_size=4, padding='same', activation='relu'))\n","\n","        CNNmodel.add(Flatten())\n","\n","        # Automatically handling the input size\n","        CNNmodel.add(Dense(64, activation='relu'))\n","        CNNmodel.add(Dense(1))\n","        CNNmodel.add(Activation('linear'))\n","\n","        CNNmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mae', metrics=['mae', 'mse'])\n","\n","        # Printing the model summary to check the shape\n","        print(CNNmodel.summary())\n","\n","        batch_size = 32\n","        epochs = 50\n","        CNN_history = CNNmodel.fit(X_train_w, r_train_w,\n","                                   batch_size=batch_size,\n","                                   epochs=epochs,\n","                                   verbose=1,\n","                                   validation_data=(X_val_w, r_val_w),\n","                                   shuffle=True)\n","\n","        histories[WINDOW] = CNN_history.history\n","\n","        y_val_CNNmodel = CNNmodel.predict(X_val_w)\n","        y_test_CNNmodel = CNNmodel.predict(X_test_w)\n","\n","        # Calculating MAE\n","        mae_val_CNNmodel = mean_absolute_error(r_val_w, y_val_CNNmodel)\n","        mae_test_CNNmodel = mean_absolute_error(r_test_w, y_test_CNNmodel)\n","\n","        # Calculating MSE\n","        mse_val_CNNmodel = mean_squared_error(r_val_w, y_val_CNNmodel)\n","        mse_test_CNNmodel = mean_squared_error(r_test_w, y_test_CNNmodel)\n","\n","        # Calculating RMSE\n","        rmse_val_CNNmodel = np.sqrt(mse_val_CNNmodel)\n","        rmse_test_CNNmodel = np.sqrt(mse_test_CNNmodel)\n","\n","        # Storing the MAE results\n","        mae_new_row = pd.DataFrame({\n","            'Look-back Period (Seconds)': [WINDOW],\n","            'Forecasting horizon (Seconds)': [AHEAD],\n","            'Validation MAE': [mae_val_CNNmodel],\n","            'Test MAE': [mae_test_CNNmodel]\n","        })\n","        mae_results = pd.concat([mae_results, mae_new_row], ignore_index=True)\n","\n","        # Storing the MSE results\n","        mse_new_row = pd.DataFrame({\n","            'Look-back Period (Seconds)': [WINDOW],\n","            'Forecasting horizon (Seconds)': [AHEAD],\n","            'Validation MSE': [mse_val_CNNmodel],\n","            'Test MSE': [mse_test_CNNmodel]\n","        })\n","        mse_results = pd.concat([mse_results, mse_new_row], ignore_index=True)\n","\n","        # Storing the RMSE results\n","        rmse_new_row = pd.DataFrame({\n","            'Look-back Period (Seconds)': [WINDOW],\n","            'Forecasting horizon (Seconds)': [AHEAD],\n","            'Validation RMSE': [rmse_val_CNNmodel],\n","            'Test RMSE': [rmse_test_CNNmodel]\n","        })\n","        rmse_results = pd.concat([rmse_results, rmse_new_row], ignore_index=True)\n","\n","        # Printing the results\n","        print(f\"Window size: {WINDOW}, Ahead: {AHEAD}\")\n","        print(f\"Validation MAE: {mae_val_CNNmodel}, Test MAE: {mae_test_CNNmodel}\")\n","        print(f\"Validation MSE: {mse_val_CNNmodel}, Test MSE: {mse_test_CNNmodel}\")\n","        print(f\"Validation RMSE: {rmse_val_CNNmodel}, Test RMSE: {rmse_test_CNNmodel}\")\n","\n","        # Saving predictions to CSV\n","        predictions_df = pd.DataFrame({\n","            'Actual': r_val_w.flatten(),\n","            'Predicted': y_val_CNNmodel.flatten()\n","        })\n","        predictions_df.to_csv(f'{folder_path}predictions_val_window_{WINDOW}_ahead_{AHEAD}.csv', index=False)\n","\n","        predictions_test_df = pd.DataFrame({\n","            'Actual': r_test_w.flatten(),\n","            'Predicted': y_test_CNNmodel.flatten()\n","        })\n","        predictions_test_df.to_csv(f'{folder_path}predictions_test_window_{WINDOW}_ahead_{AHEAD}.csv', index=False)\n","\n","        # Plotting results and history\n","        PlotResults(r_val_w[:1000], y_val_CNNmodel[:1000, 0], WINDOW, AHEAD, dataset_type=\"Validation\")\n","        PlotResults(r_test_w[:1000], y_test_CNNmodel[:1000, 0], WINDOW, AHEAD, dataset_type=\"Test\")\n","        plot_history(CNN_history)\n","\n","# Saving MAE, MSE, and RMSE results to CSV files\n","mae_results.to_csv(f'{folder_path}mae_results_summary.csv', index=False)\n","mse_results.to_csv(f'{folder_path}mse_results_summary.csv', index=False)\n","rmse_results.to_csv(f'{folder_path}rmse_results_summary.csv', index=False)\n","\n","print(\"Loop stopped\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
